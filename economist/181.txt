IN MANY Western countries, including America and Britain, divorce lawyers are not allowed to represent a client in exchange for a share of whatever settlement they secure (as opposed to charging a fixed fee). Such arrangements, it is feared, would beget more and nastier divorces. Yet the same rules do not apply to financiers: they are free to fund legal battles over marital assets—and a growing number do. Novitas Loans, a British firm, is currently lending to 1,500 would-be divorcées (most are women) or divorcés, at 18% annual interest. The loans are intended to cover legal fees; applicants typically expect to win assets worth three times their borrowing. Without the loans, many would have to give up and settle for much less, says Jason Reeve, the firm’s managing director. It gets lots of thank-you letters from borrowers, he claims. Demand for loans of this kind has jumped since the British government restricted legal aid for divorces in 2013, notes Nigel Shepherd of Resolution, an association of family lawyers. Brendan Lyle of BBL Churchill Group, an American firm with cases in 27 states, says his role is “part financier and part therapist”. Vengeful clients, determined to fight over every last teaspoon, must be restrained. The firm’s average loan is $306,000 in New York state and somewhat less elsewhere. The typical interest rate is around 16%. The default rate is a modest 2%, although there is some forbearance for struggling customers. The big risk for divorce funders is that a couple might get back together. When that happens, assets are not sold and a borrower may not be able to pay back the lender. Novitas therefore prefers to fund cases that have been grinding through court long enough to make reconciliation unlikely. The Iceberg Partnership, Novitas’s biggest competitor in Britain, transfers this risk to the 374 law firms that refer clients for funding. They must agree to guarantee their clients’ loans, repaying Iceberg themselves if the client defaults. Rather than lend money, some firms “invest” in divorce cases, asking for repayment only if a settlement is reached. A year ago Novitas bought an American firm, National Divorce Capital, that offers such “non-recourse cash advances”. Since then, applications for advances have doubled. It plans to open branches in Australia and Canada later this year.BACK in the 1970s, after American regulators abolished fixed commissions for brokers who helped their clients trade shares, the likes of Charles Schwab and Fidelity were the insurgents. They dispensed with the expensive frills that most rivals offered, such as research and investment advice. That, in turn, allowed them to offer share trading to the masses at bargain prices. Twenty years later, the internet spurred the growth of a new wave of discount brokers, including E*Trade and TD Ameritrade. Now for the next challenger. Whereas full-service brokers demand a percentage of the value of the assets in their clients’ accounts (typically 1-1.75% a year), the discount firms charge around $9 a trade. That is highway robbery, however, by the standards of a new online brokerage, Robinhood, which enables clients to trade shares free of charge, via a new mobile app. Instead of taking commissions from customers, Robinhood receives them from the trading venues to which it steers their orders, a controversial but common practice. It also earns returns from the cash clients leave in their accounts, and plans soon to offer margin trading—the buying of stock with borrowed money—for which it will charge a fee. Whether this is enough to cover Robinhood’s costs is unclear (the firm does not disclose its financial results). But it provides an even leaner service than its rivals. It does not yet offer trading on its website, for example, catering to clients, whose average age is 28, exclusively via its app. Robinhood is also easy to use. Setting up an account can take as little as four minutes. To confirm customers’ identity, the firm asks them to take a picture of an ID with a smartphone. That has helped it to attract almost 1m customers since it started operations a year ago. It says it continued to recruit new clients during the recent market turmoil, even as activity declined at other firms. To bring in even more custom, it has begun integrating its service with apps such as Stocktwits, a social-media platform for retail investors. Increased turnover should boost its income more than its costs. In time, it may make life difficult for the disruptors of yore.TWO : NIL to the computer. That was the score, as The Economist went to press, in the latest round of the battle between artificial intelligence (AI) and the naturally evolved sort. The field of honour is a Go board in Seoul, South Korea—a country that cedes to no one, least of all its neighbour Japan, the title of most Go-crazy place on the planet. To the chagrin of many Japanese, who think of Go as theirs in the same way that the English think of cricket, the game’s best player is generally reckoned to be Lee Sedol, a South Korean. But not, perhaps, for much longer. Mr Lee is in the middle of a five-game series with AlphaGo, a computer program written by researchers at DeepMind, an AI software house in London that was bought by Google in 2014. And, though this is not an official championship series, as the scoreline shows, Mr Lee is losing. Go is an ancient game—invented, legend has it, by the mythical First Emperor of China, for the instruction of his son. It is played all over East Asia, where it occupies roughly the same position as chess does in the West. It is popular with computer scientists, too. For AI researchers in particular, the idea of cracking Go has become an obsession. Other games have fallen over the years—most notably when, in 1997, one of the best chess players in history, Garry Kasparov, lost to a machine called Deep Blue. Modern chess programs are better than any human. But compared with Go, teaching chess to computers is a doddle. At first sight, this is odd. The rules of Go are simple and minimal. The players are Black and White, each provided with a bowl of stones of the appropriate colour. Black starts. Players take turns to place a stone on any unoccupied intersection of a 19x19 grid of vertical and horizontal lines. The aim is to use the stones to claim territory. In the version being played by Mr Lee and AlphaGo each stone, and each surrounded intersection, is a point towards the final score. Stones surrounded by enemy stones are captured and removed. If an infinite loop of capture and recapture, known as Ko, becomes possible, a player is not allowed to recapture immediately, but must first play elsewhere. Play carries on until neither player wishes to continue. Go forth and multiply This simplicity, though, is deceptive. In a truly simple game, like noughts and crosses, every possible outcome, all the way to the end of a game, can be calculated. This brute-force approach means a computer can always work out which move is the best in a given situation. The most complex game to be “solved” this way is draughts, in which around 1020 (a hundred billion billion) different matches are possible. In 2007, after 18 years of effort, researchers announced that they had come up with a provably optimum strategy. But a draughts board is only 8x8. A Go board’s size means that the number of games that can be played on it is enormous: a rough-and-ready guess gives around 10170. Analogies fail when trying to describe such a number. It is nearly a hundred of orders of magnitude more than the number of atoms in the observable universe, which is somewhere in the region of 1080. Any one of Go’s hundreds of turns has about 250 possible legal moves, a number called the branching factor. Choosing any of those will throw up another 250 possible moves, and so on until the game ends. As Demis Hassabis, one of DeepMind’s founders, observes, all this means that Go is impervious to attack by mathematical brute force. But there is more to the game’s difficulty than that. Though the small board and comparatively restrictive rules of chess mean there are only around 1047 different possible games, and its branching factor is only 35, that does, in practice, mean chess is also unsolvable in the way that draughts has been solved. Instead, chess programs filter their options as they go along, selecting promising-looking moves and reserving their number-crunching prowess for the simulation of the thousands of outcomes that flow from those chosen few. This is possible because chess has some built-in structure that helps a program understand whether or not a given position is a good one. A knight is generally worth more than a pawn, for instance; a queen is worth more than either. (The standard values are three, one and nine respectively.) Working out who is winning in Go is much harder, says Dr Hassabis. A stone’s value comes only from its location relative to the other stones on the board, which changes with every move. At the same time, small tactical decisions can have, as every Go player knows, huge strategic consequences later on. There is plenty of structure—Go players talk of features such as ladders, walls and false eyes—but these emerge organically from the rules, rather than being prescribed by them. Since good players routinely beat bad ones, there are plainly strategies for doing well. But even the best players struggle to describe exactly what they are doing, says Miles Brundage, an AI researcher at Arizona State University. “Professional Go players talk a lot about general principles, or even intuition,” he says, “whereas if you talk to professional chess players they can often do a much better job of explaining exactly why they made a specific move.” Intuition is all very well. But it is not much use when it comes to the hyper-literal job of programming a computer. Before AlphaGo came along, the best programs played at the level of a skilled amateur. Go figure AlphaGo uses some of the same technologies as those older programs. But its big idea is to combine them with new approaches that try to get the computer to develop its own intuition about how to play—to discover for itself the rules that human players understand but cannot explain. It does that using a technique called deep learning, which lets computers work out, by repeatedly applying complicated statistics, how to extract general rules from masses of noisy data. Deep learning requires two things: plenty of processing grunt and plenty of data to learn from. DeepMind trained its machine on a sample of 30m Go positions culled from online servers where amateurs and professionals gather to play. And by having AlphaGo play against another, slightly tweaked version of itself, more training data can be generated quickly. Those data are fed into two deep-learning algorithms. One, called the policy network, is trained to imitate human play. After watching millions of games, it has learned to extract features, principles and rules of thumb. Its job during a game is to look at the board’s state and generate a handful of promising-looking moves for the second algorithm to consider. This algorithm, called the value network, evaluates how strong a move is. The machine plays out the suggestions of the policy network, making moves and countermoves for the thousands of possible daughter games those suggestions could give rise to. Because Go is so complex, playing all conceivable games through to the end is impossible. Instead, the value network looks at the likely state of the board several moves ahead and compares those states with examples it has seen before. The idea is to find the board state that looks, statistically speaking, most like the sorts of board states that have led to wins in the past. Together, the policy and value networks embody the Go-playing wisdom that human players accumulate over years of practice. As Mr Brundage points out, brute force has not been banished entirely from DeepMind’s approach. Like many deep-learning systems, AlphaGo’s performance improves, at least up to a point, as more processing power is thrown at it. The version playing against Mr Lee uses 1,920 standard processor chips and 280 special ones developed originally to produce graphics for video games—a particularly demanding task. At least part of the reason AlphaGo is so far ahead of the competition, says Mr Brundage, is that it runs on this more potent hardware. He also points out that there are still one or two hand-crafted features lurking in the code. These give the machine direct hints about what to do, rather than letting it work things out for itself. Nevertheless, he says, AlphaGo’s self-taught approach is much closer to the way people play Go than Deep Blue’s is to the way they play chess. One reason for the commercial and academic excitement around deep learning is that it has broad applications. The techniques employed in AlphaGo can be used to teach computers to recognise faces, translate between languages, show relevant advertisements to internet users or hunt for subatomic particles in data from atom-smashers. Deep learning is thus a booming business. It powers the increasingly effective image- and voice-recognition abilities of computers, and firms such as Google, Facebook and Baidu are throwing money at it. Deep learning is also, in Dr Hassabis’s view, essential to the quest to build a general artificial intelligence—in other words, one that displays the same sort of broad, fluid intelligence as a human being. A previous DeepMind paper, published in 2015, described how a computer had taught itself to play 49 classic Atari videogames—from “Space Invaders” to “Breakout”—simply by watching the screen, with no helpful hints (or even basic instructions) from its human overlords. It ended up doing much better than any human player can. (In a nice coincidence, atari is also the name in Go for a stone or group of stones that is in peril of being captured.) Games offer a convenient way to measure progress towards this general intelligence. Board games such as Go can be ranked in order of mathematical complexity. Video games span a range of difficulties, too. Space Invaders is a simple game, played on a low-resolution screen; for a computer to learn to play a modern video game would require it to interpret a picture much more subtle and complicated than some ugly-looking monsters descending a screen, and in pursuit of much less obvious goals than merely zapping them. One of DeepMind’s next objectives, Dr Hassabis says, is to build a machine that can learn to play any game of cards simply by watching videos of humans doing so. Go tell the Spartans For now, he reckons, general-purpose machine intelligence remains a long way off. The pattern-recognising abilities of deep-learning algorithms are impressive, but computers still lack many of the mental tools that humans take for granted. A big one is “transfer learning”, which is what AI researchers call reasoning by analogy. This is the ability to take lessons learned in one domain and apply them to another. And machines like AlphaGo have no goals, and no more awareness of their own existence than does a word processor or a piece of accounting software. In the short term, though, Dr Hassabis is optimistic. At a kiwon, or Go parlour, in Seoul, the day before the match, the 30 or so players present were almost unanimous in believing that the machine would fall short. “Lee is a genius who is constantly creating new moves; what machine can replicate that?” asked one. At a pre-match press conference Mr Lee said he was confident he would win 5-0, or perhaps 4-1. He was, plainly, wrong about that, although it is not over yet. “He’s a very good player,” said a diplomatic Dr Hassabis before the match. “But our internal tests say something different.” Even if Mr Lee does manage to pull off an improbable victory, though, humans are unlikely to stay on top for long. As AlphaGo’s algorithms are tweaked, and as it gathers more data from which to learn, it is only going to get better. Asked whether there was a ceiling to its abilities, Dr Hassabis said he did not know: “If there is, we haven’t found it yet.” Correction: An earlier version of this story suggested that 10170 was the number of possible positions of stones on a Go board; in fact it is an estimate of the number of possible Go games. SorrySomething to chew on IN 2009 Richard Wrangham, an anthropologist at Harvard, published an intriguing thesis. He was trying to answer a question that had long puzzled workers in his field: how could the evolution of an organ as energetically expensive to sustain as the human brain have happened? Before Dr Wrangham’s work the conventional answer was: “meat-eating”. Archaeological evidence such as a lack of tool marks on animal bones suggests humanity’s ancestors, the Australopithecines, were largely vegetarian. By contrast Homo erectus, the first widespread human being (pictured below), also ate meat, which is a more compact source of calories than most plant matter, and might thus have provided the extra brain-food needed. Dr Wrangham, however, had a different answer: “cooking”. He showed that the ease of digestion and additional nutritional value made available by treating food with fire (which alters starch and protein molecules in ways that make them easier to digest) boosts its calorific value enough for a reasonable daily intake to power both brain and body—so much so that modern humans who attempt to live only on raw foodstuffs (there are a few who try) have great difficulty remaining well-nourished. On top of this, the softening brought about by cooking could explain a second evolutionary trend, that toward smaller teeth and less-powerful jaws. Just when Homo erectus did start cooking is controversial. The oldest definitive evidence dates back only 500,000 years, though the species evolved 1.9m years ago. But the Wrangham thesis does not depend only on the beginning of heat-treating food. It also includes food preparation using tools to chop or pound meat and vegetables. This presumably makes them easier to digest. It also makes them easier to chew, which might account for the reduction in jaw and tooth size. A paper published in this week’s Nature by Katherine Zink and Daniel Lieberman, two of Dr Wrangham’s colleagues at Harvard, brings some evidence to bear on these questions, particularly that of chewing. Dr Zink and Dr Lieberman used replicas of the stone tools available to Homo erectus to process food, and looked at the consequences for those who attempted to masticate the result. The pseudo-Palaeolithic diet the two researchers chose comprised beets, carrots and yams as root vegetables, and goat as meat. They prepared the vegetables four ways: raw and unprocessed; raw and hit six times with a copy of a Palaeolithic hammerstone; raw and cut into small slices; and roasted for 15 minutes. The goat was also served four ways: raw and unprocessed; raw and pounded 50 times by a hammerstone; raw and cut into small slices; and cooked on a grill for 25 minutes. Dr Zink and Dr Lieberman then fed each preparation to a group of volunteers, to see how easy it was to chew. To measure this, they wired up the skin of their volunteers’ jaws using electrodes which recorded the force a volunteer exerted chewing. Once wired, volunteers were given samples to chew and asked to do so until they felt what they were chewing was ready to swallow. Sometimes the volunteers were then allowed to swallow. On other occasions, though, they were asked to spit the sample out, so that the bits could be analysed. (The raw meat was always spat out, to prevent foodborne illness.) Dr Zink and Dr Lieberman found, in line with Dr Wrangham’s original thesis, that chewing cooked root vegetables required a third less force than was needed to chew an equivalent amount of raw and unprocessed root. Slicing the vegetables did not provide any benefit, but pounding them reduced the force required to chew by about 9%. Pounding meat, by contrast, brought no benefit, whereas slicing it did. As with cooking the vegetables, it reduced the chewing force needed by around a third. Intriguingly, roasting meat actually increased the masticatory force required. On top of this, when Dr Zink and Dr Lieberman examined food spat out by their volunteers at the point it was deemed ready to swallow, they found that the unprocessed and the pounded meat usually came back as a single large lump that would be hard for the gut to break down. In contrast, when the meat was sliced or cooked before being chewed, participants were consistently able to chew it into tiny, digestible particles. Putting all their results together, Dr Zink and Dr Lieberman conclude that a diet of one-third sliced meat and two-thirds pounded vegetables, such as Homo erectus might reasonably have been expected to consume even in the absence of fire, would need 27% less effort to chew than an unpounded all-vegetable diet. Specifically, the inclusion of meat contributed a 15% reduction and the slicing and pounding a 12% reduction, which Dr Lieberman calculates equates to 2.5m fewer chews a year. That could certainly account for the shrinkage of jaws and teeth undergone by Homo erectus. As to its consequences farther down the digestive tract, those remain the province of further research.ASSANE GUEYE, a Senegalese-born postdoctoral researcher at America’s National Institute of Standards and Technology, is a student of systems. He studies the multiple networks of communications that hold cities together, and feels that a new scientific discipline is needed to describe these systems of systems. He hopes to create one. Amanda Weltman, a physicist at the University of Cape Town, seeks nuance in the laws of gravity. She suspects there is an undiscovered particle that links gravitational attraction with nature’s other forces, and is planning an experiment that uses a special satellite to try to track it down. Tolu Oni, an epidemiologist also at Cape Town, and Evelyn Gitau, an immunologist at the Kenya Medical Research Institute, both know that the seriousness of the illnesses they are trying to beat—AIDS, malaria and tuberculosis—can be amplified or diminished by patients’ circumstances, but they do not understand the details. Both have the same problem, managing large data sets. But until this week, they had never met. These four scientists are among 15 fellows who, together with 800 other academics, business folk and politicians (including the presidents of Senegal and Rwanda), are gathered at the Next Einstein Forum, being held this week in Dakar in Senegal. Next Einstein, the brainchild of Thierry Zomahoun, a Béninois administrator, is an attempt to scale up African science. At the moment, most African scientists work either in isolation or abroad. They do not know one another and are invisible to prospective colleagues. Visas for collaborative work are hard to come by and even flying from one part of Africa to another may require going via a non-African hub, such as Dubai. Next Einstein is an attempt to overcome this fragmentation, by providing a continental congress at which African scientists can meet. The forum has grown out of the African Institute for Mathematical Sciences (AIMS), of which Mr Zomahoun is president. AIMS is a graduate school with branches in Cameroon, Ghana, Tanzania and South Africa, as well as in Senegal. It was founded in 2003 by Neil Turok, a South African who now directs the Perimeter Institute for Theoretical Physics. In a sense, Dr Turok’s situation is an example of the problems Next Einstein is trying to overcome, for the Perimeter is based in Canada, and so he now lives in Ontario. AIMS concentrates on maths for two reasons. First, being a subject that requires little equipment beyond students’ brains, it is cheap to teach. Second, when Dr Turok was asking fellow African researchers which subjects would be most pertinent to the continental scientific Renaissance he hoped to trigger, most agreed that maths, which is fundamental to the rest of science, was the one to go for. The Senegalese campus, built on a beach-front an hour’s drive south of Dakar, is a place where up to 80 graduate students (who pay no fees for the privilege, the institute being funded by government grants and commercial sponsors) can sit for a year at the feet of visiting academics from Africa and elsewhere. These visitors come for three-week stints to lecture in subjects such as cryptography, finance and quantum mechanics. Students are thus exposed to a wide variety of ideas during their stay. Whether Next Einstein, which plans to meet every two years, will be able to build on AIMS and create for African scientists the sorts of opportunities that those in the rich world take for granted remains to be seen. But, as Mr Zomahoun observes, 40% of the world’s children are African. Statistically, therefore, the chances that the next Einstein will come from Africa are good. This week’s meeting in Dakar is at least a step on the road to making those odds more than just theoretical.  Just call me Jack HANDING a farmer a fistful of magic beans with the promise that they will improve his business might sound like something out of a fairy-tale. But, as Arthur C. Clarke put it, any sufficiently advanced technology is indistinguishable from magic. The sensor-filled “beans” developed by Andrew Holland, an electronics engineer from Swaffham Bulbeck, near Cambridge, England, are not only advanced technology. They could also, Mr Holland says, provide an answer to many a farmer’s prayers. Mixed into the contents of a granary, his beans would report continuously on the temperature and humidity, both of which encourage rotting if they are too high, and on carbon-dioxide levels, which reflect the amount of insect breath exhaled, and thus the level of infestation. At the moment these things have to be measured (if they are measured at all) using hand-held instruments that are plunged into the grain pile at regular intervals by farmhands.   The beans themselves are plastic shells 45mm long and 18mm wide, manufactured by 3D printing. This process is used to encapsulate within each bean a diminutive circuit board containing a low-power Bluetooth radio and sensors that can measure motion, temperature, humidity, air pressure and the concentrations of several gases, including carbon dioxide and carbon monoxide. A bean also contains an electronic compass and a tiny gyroscope that, acting together, sense its orientation. All of these devices are powered by a wirelessly rechargeable battery. Mr Holland sees potential for his device beyond the monitoring of stored crops. Placed discreetly in a living room or office, he suggests, it could register intruders via the trembles of its motion sensor. A change in air pressure brought about by blowing on it might let it work as a switch for a room’s lights. The gyroscope would permit it to act as the remote control for a television or hi-fi: swiping a bean through the air could turn the device on, while spinning it in a circle could step the volume up or down, depending on whether the spin were clockwise or anticlockwise. For the elderly, a bean carried in a pocket could register a fall and then call for help via its owner’s phone. For the suspicious, it could record whether a parcel had been mistreated in transit by being heated up or crushed. That beans would be better than existing ways of doing these things is not always obvious. But they will be programmable via a phone app, so owners will be able to devise other uses as they see fit. Grain-monitoring, though, is likely to be the first use. Once placed in and around a heap of grain, a collection of the beans will connect together wirelessly, becoming nodes in a network that gives a clear, three-dimensional picture of what is going on inside that heap. Mr Holland’s company, RFMOD, has just started testing beans for this purpose, and he hopes they will be commercially deployed within two years. One problem is recovering the beans when the granary is emptied. If they became a routine technology this could, no doubt, be done by “pinging” them when a shipment was sorted at the wholesaler, and pulling them out automatically as the grain left a hopper. In the meantime, RFMOD is experimenting with putting them in the plastic insect-trapping containers that farmers already deploy in grain-piles to keep infestations under control. If the beans do well at monitoring grain, Mr Holland hopes their other applications will make them an important part of the much-discussed “internet of things” which some prophets believe will, in the future, link many objects not currently connected electronically. If his own wildest dreams are fulfilled, that would make RFMOD a large and successful company. It might also suggest that Swaffham Bulbeck, a tiny village, has its own brand of magic to confer, for it was also once home to another startup, Advanced RISC Machines Ltd. ARM Holdings, as that firm is now known, has grown into one of the world’s biggest designers of microprocessors. In Silicon Valley, they do it in garages. In the English fens, it seems, old barns are just as good.
