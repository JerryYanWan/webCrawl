DIVIDENDS provide the vast bulk of long-term returns from equities. Work by Elroy Dimson, Paul Marsh and Mike Staunton of the London Business School shows that the real annual total return from American shares since 1900 has been 6.4%. Capital gains supplied just a third of that figure; reinvested dividends accounted for the rest. So the outlook for dividends ought to be crucial for equity investors. They should be concerned that, in some markets, dividend income is concentrated in a small number of stocks (see chart). In Australia, Britain, France, Germany and Switzerland, more than 70% of the dividends come from just 20 companies. That leaves investors’ income dependent on the fortunes of just a few industries. Banks were big dividend-payers until the financial crisis of 2008; energy and mining companies have been good sources of income since then. But falling commodity prices are leading energy companies to reduce their payouts. Last year 504 American companies cut their dividends, according to Standard & Poor’s, a credit-rating agency, compared with 291 in 2014. Energy companies made up nearly half of the dividend-cutting group in the fourth quarter. As a result of these cuts, dividends are growing more slowly than before. In the fourth quarter of 2015, dividends rose by $3.6 billion in cash terms, compared with a $12 billion increase in the same period of 2014. Investors who need income are now relying on the pharmaceutical and health-care sectors; research by Andrew Lapthorne of Société Générale, a bank, shows that the three largest stock holdings of global income funds are Pfizer, Roche and Johnson & Johnson. The narrow base of dividend provision is important when it comes to judging the attractiveness of equities. In some markets, dividend yields are higher than government-bond yields; in Britain, for example, the FTSE All-Share index yields 4% whereas 10-year gilts offer just 1.7%. For some, this makes equities a bargain. Until the 1950s it was the norm for equities to have a higher yield than bonds. Shares were perceived to be riskier than government bonds so investors demanded higher payouts for owning them. But opinion changed as the market began to be dominated by institutional investors—pension funds and insurance companies. Their size allowed them to own diversified portfolios, in which the consequences of the failure of an individual firm were much reduced. Thus hedged, they piled into equities to capitalise on the tendency of dividends to grow over time. Interest payments on bonds, in contrast, are fixed, which was a particular problem in the inflationary environment of the 1960s and 1970s. As a result the dividend yield dropped below the government-bond yield in most markets and stayed there. Since the financial crisis of 2008, the ratio seems to have undergone another fundamental shift. Government bonds are valued for their safety, particularly in a world of low inflation. A high yield on an equity, meanwhile, may simply suggest that investors expect the dividend to be cut. Shares in BHP Billiton, a mining group, have plunged along with commodity prices. That makes their yield, calculated using last year’s dividend, look extremely high, at 12%. But analysts expect the dividend to be cut in half this year. Investors may also be seeking a higher overall dividend yield on equities to reflect the riskier nature of the income stream now that dividends are more concentrated among fewer companies. What about share buy-backs? They are an alternative source of income for investors; for some, they are a more tax-efficient way of receiving cash. But buy-backs are much more variable than dividends: the amount spent on them by non-financial companies in the S&P 500 index fell from over $400 billion in 2007 to under $70 billion in 2009, according to Deutsche Bank. Companies can quietly trim their buy-back programmes; a dividend cut is a public sign of trouble. And, of course, investors who sell their shares in a buy-back need to find some other asset to replace that source of income. Investors ignore dividends at their peril. In more than a century of data, across 19 countries, the LBS academics found that annual returns from the markets with the highest dividend yields were eight percentage points higher than those from the lowest-yielding markets. So during the current reporting season, smart investors will be looking not just at notional earnings (which can be a highly subjective measure) but at the cold, hard cash that companies are shelling out.ONE of the supposed virtues of peer-to-peer lenders—websites that connect borrowers to people with money to lend—is transparency. They often publish a range of information about those seeking loans (credit history, employment status, income), so that the investors stumping up the money know what they are getting into. So it is fitting that Imperial Investment, a Chinese P2P firm, is impressively transparent about its own circumstances. Earlier this month it published four separate notices from police, employees and family pleading for its runaway founder to return. “Our faces are bathed in tears,” the employees wrote. Chinese media were far more phlegmatic about the woes of Imperial Investment, which has facilitated 935m yuan ($142m) in loans since its launch in 2013. “Runaway P2P bosses are no longer newsworthy,” declared the Jinling Evening News. At the end of 2015, nearly a third of all Chinese P2P lenders (1,263 out of 3,858) had run into difficulties, according to Online Lending House, an industry website. It classifies them according to the nature of their troubles: halted operations, disputes, frozen withdrawals or, as in the case of Imperial, bosses who have absconded. Running away may sound rather extreme but it turns out to be popular: 266 P2P bosses have fled over the past six months, by Online Lending House’s count. Although most of the firms in trouble are small, a few bigger ones have also come unstuck: Ezubao, China’s biggest P2P lender, which has arranged $11 billion-worth of loans, is one of the firms with frozen accounts. Chinese P2P lenders’ many and varied problems might be expected to deter investors. Yet some of the bigger, better-run firms are still attracting serious money. In December Yirendai, the consumer arm of P2P lender CreditEase, became the first Chinese “fintech” firm to go public abroad, listing on the New York Stock Exchange with a valuation of around $585m. Earlier this month Lufax, a platform for a range of products including P2P loans, completed a fundraising round that valued it at $18.5 billion, setting it up for a keenly anticipated IPO. Both companies pride themselves on their risk controls. Daily dispatches: China's market mess The optimistic scenario is that well-managed fintech firms will bring much-needed competition and efficiency to China’s sclerotic banking system, and profit handsomely while at it. The biggest lenders in China are mammoth state-owned banks, which tend to favour lending to state-owned enterprises over lending to private firms. That cedes plenty of space to P2P firms to build up their customer base and deliver credit to previously overlooked segments of the economy. The worry, though, is that the sudden rush of money into P2P could push even good firms into bad lending decisions. Outstanding P2P credit rose more than tenfold over the past two years, from 31 billion yuan at the start of 2014 to 439 billion yuan at the end of last year. Average lending rates, meanwhile, fell from nearly 20% to 12.5%. Should inflows to P2P firms slow, lending rates will not be the only thing to spike higher: so too will the incidence of runaway bosses.FOR a spell last year American banks seemed poised to reattain the sort of double-digit returns that have largely eluded them since the financial crisis. A robust market for takeovers and public offerings was producing a flurry of fees. Credit quality, which had collapsed in the crisis, was “pristine”, according to Jamie Dimon, the boss of JPMorgan Chase, America’s biggest bank by assets—something that was allowing banks to reduce the provisions they had made to cover soured loans. The rash of swingeing fines that had been disfiguring profits had largely dissipated (although Goldman Sachs recently agreed to pay $5 billion to settle charges that it knowingly peddled dodgy mortgage-backed securities). And then there was the Federal Reserve’s decision to raise interest rates in December for the first time in nearly a decade, which held out the prospect of a growing margin between the rates banks pay depositors and those they charge borrowers. Glimmers of that sunnier outlook can still be seen in the big American banks’ annual results. JPMorgan Chase reported a record annual profit on January 14th of $24 billion. Bank of America and Citigroup posted their biggest profits since 2006 (although the return on equity at both is a fraction of what it once was). Yet share prices in the banking sector have fallen by more than 20% since July, with half of the decline coming in the first weeks of 2016. In part, the poor performance of bank shares stems from the broader gloom in global markets. But investors have also noticed that the growth in banks’ profits comes more from falling costs than from rising revenue. Worse, the trends that propelled profits upwards in 2015 appear to be reversing. Analysts have cut their forecasts for profits in the coming year (see chart). The worsening outlook for the world economy has made markets much more sceptical that the Fed will continue raising rates. Whereas members of the Fed’s rate-setting committee predicted in December that rates would rise by a full percentage point this year, markets now expect an increase of only a quarter of a point. The prospect of higher lending margins, in other words, is evaporating. Yet December’s increase has curbed the hugely profitable business of refinancing mortgages. The volatility in markets, meanwhile, is causing takeovers and issuance of shares and debt to atrophy. There is supposed to be a bright side to the turmoil, since volatility typically boosts trading revenues. But many investment banks have curtailed their trading operations under regulatory pressure. That has left them ill placed to capitalise on the turmoil. Banks have been beefing up wealth-management arms even as they curb trading, in the hope they will provide steadier profits at less risk. But falling markets also harm these, since costs are fixed but revenues come in the form of a percentage of the shrinking value of assets under management. In addition, instead of reducing provisions, banks are now adding to them. The main culprit is the collapsing oil price, which is crushing energy firms. JPMorgan Chase, for example, set aside $124m in the final quarter of last year to cover any losses in its loans to energy firms. None of these problems is fatal. According to Goldman Sachs, Citi has the biggest exposure to energy firms among banking behemoths, at a modest 3.3% of its loan book. Recent years have been lean in part because banks have been building up their buffers rather than racking up big profits. Although many banks struggle to earn a decent return, the number that are failing or in trouble is near a record low, according to the Federal Deposit Insurance Corporation, a regulator. For investors, however, that is scant consolation.DISMAL may not be the most desirable of modifiers, but economists love it when people call their discipline a science. They consider themselves the most rigorous of social scientists. Yet whereas their peers in the natural sciences can edit genes and spot new planets, economists cannot reliably predict, let alone prevent, recessions or other economic events. Indeed, some claim that economics is based not so much on empirical observation and rational analysis as on ideology. In October Russell Roberts, a research fellow at Stanford University’s Hoover Institution, tweeted that if told an economist’s view on one issue, he could confidently predict his or her position on any number of other questions. Prominent bloggers on economics have since furiously defended the profession, citing cases when economists changed their minds in response to new facts, rather than hewing stubbornly to dogma. Adam Ozimek, an economist at Moody’s Analytics, pointed to Narayana Kocherlakota, president of the Federal Reserve Bank of Minneapolis from 2009 to 2015, who flipped from hawkishness to dovishness when reality failed to affirm his warnings of a looming surge in inflation. Tyler Cowen, an economist at George Mason, published a list of issues on which his opinion has shifted (he is no longer sure that income from capital is best left untaxed). Paul Krugman, an economist and New York Times columnist, chimed in. He changed his view on the minimum wage after research found that increases up to a certain point reduced employment only marginally (this newspaper had a similar change of heart). Economists, to be fair, are constrained in ways that many scientists are not. They cannot brew up endless recessions in test tubes to work out what causes what, for instance. Yet the same restriction applies to many hard sciences, too: geologists did not need to recreate the Earth in the lab to get a handle on plate tectonics. The essence of science is agreeing on a shared approach for generating widely accepted knowledge. Science, wrote Paul Romer, an economist, in a paper* published last year, leads to broad consensus. Politics does not. Nor, it seems, does economics. In a paper on macroeconomics published in 2006, Gregory Mankiw of Harvard University declared: “A new consensus has emerged about the best way to understand economic fluctuations.” But after the financial crisis prompted a wrenching recession, disagreement about the causes and cures raged. “Schlock economics” was how Robert Lucas, a Nobel-prize-winning economist, described Barack Obama’s plan for a big stimulus to revive the American economy. Mr Krugman, another Nobel-winner, reckoned Mr Lucas and his sort were responsible for a “dark age of macroeconomics”. As Mr Roberts suggested, economists tend to fall into rival camps defined by distinct beliefs. Anthony Randazzo of the Reason Foundation, a libertarian think-tank, and Jonathan Haidt of New York University recently asked a group of academic economists both moral questions (is it fairer to divide resources equally, or according to effort?) and questions about economics. They found a high correlation between the economists’ views on ethics and on economics. The correlation was not limited to matters of debate—how much governments should intervene to reduce inequality, say—but also encompassed more empirical questions, such as how fiscal austerity affects economies on the ropes. Another study found that, in supposedly empirical research, right-leaning economists discerned more economically damaging effects from increases in taxes than left-leaning ones. That is worrying. Yet is it unusual, compared with other fields? Gunnar Myrdal, yet another Nobel-winning economist, once argued that scientists of all sorts rely on preconceptions. “Questions must be asked before answers can be given,” he quipped. A survey conducted in 2003 among practitioners of six social sciences found that economics was no more political than the other fields, just more finely balanced ideologically: left-leaning economists outnumbered right-leaning ones by three to one, compared with a ratio of 30:1 in anthropology. Moreover, hard sciences are not immune from ideological rigidity. A recent study of academic citations in the life sciences found that the death of a celebrated scientist precipitates a surge in publishing from academics who previously steered clear of the celebrity’s area of study. Tellingly, papers by newcomers are cited far more heavily than new work by the celebrity’s former collaborators. That suggests that shifts of opinion in science occur not through the changing of minds so much as the displacement of one set of dogged ideologues by another. Agree to agree But even if economics is not uniquely ideological, its biases are often more salient than those within chemistry. Economists advise politicians on all manner of important decisions. A reputation for impartiality could improve both perceptions of the field and the quality of economic policy. Achieving that requires better mechanisms for resolving disputes. Mr Romer’s paper decried the pretend “mathiness” of many economists: the use of meaningless number-crunching to give a veneer of academic credibility to near-useless theories. Sifting out the guff requires transparency, argued John Cochrane of the University of Chicago in another recent blog post. Too many academics keep their data and calculations secret, he reckoned, and too few journals make space for papers that seek to replicate earlier results. Economists can squabble all they like. But the profession is of little use to anyone if it cannot then work out which side has the better of the argument.   Sources: "Mathiness in the theory of economic growth", Paul Romer, American Economic Review, Papers and Proceedings, 2015. "The macroeconomist as scientist and engineer", Gregory Mankiw, Journal of Economic Perspectives, 2006. "The moral narratives of economists", Anthony Randazzo and Jonathan Haidt, Econ Journal Watch, 2015. "Political language in economics", Zubin Jelveh, Bruce Kogut and Suresh Naidu, Columbia Business School Research Paper Number 14-57, 2015. "How politically diverse are the social sciences and humanities? Survey evidence from six fields", Daniel Klein and Charlotta Stern, Academic Questions, 2004. "Does science advance one funeral at a time?", Pierre Azoulay, Christian Fons-Rosen and Joshua Graff Zivin, NBER Working Paper 21788, 2015.Award: Philip Coggan, our Buttonwood columnist, was named “Journalist of the Year” by the CFA Society of the UK for his article “What’s Wrong with Finance”.FOR 60m years of Earth’s history, a period known to geologists as the Carboniferous, dead plants seemed unwilling to rot. When trees expired and fell to the ground, much of which was swampy in those days, instead of being consumed by agents of decay they remained more or less intact. In due course, more trees fell on them. And more, and yet more. The buried wood, pressed by layers of overburden and heated from below by the Earth’s interior, gradually lost its volatile components and was transformed into a substance closer and closer to pure carbon. The result was the coal that fuelled the Industrial Revolution, providing power for factories and railways, gas for lighting, a reducing agent for turning ore into iron and steel, the raw ingredients for drugs, dyes and other chemicals, and the energy that has generated most of the world’s electricity. Yet the abundance of Carboniferous coal is a puzzle. Forests began in the Devonian, the period before the Carboniferous, and have existed ever since. Not all coal is Carboniferous but, as the chart shows, the spike in coal accumulation then was far higher than anything which happened subsequently. Indeed, the very name Carboniferous alludes to this fact. So why, the curious ask, was it then in particular that so much coal was created? The swamps certainly helped. Lacking oxygen, they would have slowed the activities of wood-destroying micro-organisms. But swamps are not uniquely Carboniferous. To explain the special boost coal got in this period, it has been suggested that the micro-organisms around at the time were not up to the job of rotting wood. Changes in plant chemistry which let trees grow tall, this hypothesis goes, stymied these micro-organisms, making much plant material indestructible. It is an intriguing idea. But a paper just published in the Proceedings of the National Academy of Sciences, by Kevin Boyce of Stanford University and his colleagues, takes issue with it. Instead, Dr Boyce thinks abundant Carboniferous coal, swamps and all, is an accident caused by the movement of the continents. Reach for the skies The idea that Carboniferous micro-organisms could not properly digest wood depends on a hypothetical evolutionary time lag. The first vascular plants (those with internal channels to move water around) evolved in the Silurian, the period before the Devonian. Vascularisation meant a plant could suck water up its stem, and thus grow tall. This led to a race, conducted throughout the Devonian, to be tallest and thus able to capture light without being overshadowed. The consequence was trees—and therefore forests. Trees have to be strong, though, otherwise they will collapse. Part of their strength comes from cellulose, an ancient material composed of long chains of sugar molecules, which forms the walls of plant cells. But what really encouraged trees’ evolution was the advent of a second molecule, lignin. This is made of phenols, and phenols are much harder to digest than sugars—so hard, the thinking goes, that it took until after the Carboniferous was over for organisms that could do so to evolve. Meanwhile, the fallen forests simply piled up in the swamps. Though some of their cellulose was consumed, their lignin hung around and became coal. That thought is supported by analysis of the evolution of fungi. Molecular clocks, which measure rates of genetic change, suggest lignin-digesting enzymes did indeed first appear in this group (which are the main agents of rotting) in the Permian, the period immediately following the Carboniferous. Dr Boyce and his colleagues, however, do not believe it. Their disbelief is based on a painstaking analysis of Macrostrat, a database of all that is known about the stratigraphy of North America, together with an examination of which types of plant dominated the floras of stratigraphic units containing a lot of coal. The trees of the Carboniferous were not like those of today. Moreover, which types of tree predominated varied over the vast span of time that it covered. One pertinent observation Dr Boyce and his team make is that the peak of coal formation coincided with the dominance of a group called the lycopsids. Yet lycopsid trunks were composed mostly of tissue called periderm, which corresponds to modern bark and contains little lignin. Forests that existed both before and after these lycopsid woods (but before the supposed evolution of lignin-digesting fungi) had many more lignin-rich species in them, but have yielded far less coal. Moreover, though Permian rocks in North America do not contain much coal, those in China do. That does not seem consistent with idea that lignin-consumption rates suddenly increased. And, although the fossil record cannot show which enzymes were present in fungi in the past, it does show that fungi were just as diverse and active in the Carboniferous as in the Permian. Altogether, then, the abundant coal of the Carboniferous does not seem to be the result of lackadaisical fungal effort. So, in Dr Boyce’s view, the evolutionary-delay hypothesis simply will not do. Destroying a hypothesis is one thing. But it also helps if you have something to put in its place. And Dr Boyce and his colleagues have one on offer. They think the Carboniferous coal measures were a consequence of continental drift. During the Carboniferous, the continents were moving around quite a bit. Such movement, particularly when it involves continents colliding (which it did), warps them. That causes mountains and basins to form. It is the basins which interest Dr Boyce. The downwarping that created them meant they would have flooded regularly, bringing sediment that buried the tree-laden bogs, preserving them not so much from micro-organisms as from erosion. That local subsidence happened during the Carboniferous is not news. Geologists of the 19th century concluded as much—though they knew nothing of continental drift. But previous explanations for abundant coal, such as the evolutionary-lag one, have tended to concentrate on biology. Dr Boyce is suggesting that the actual cause was geological. Buried by subsidence, the coal could not be eroded, and thus survived to the present day. During the Permian, however, continental movement ceased for a time, as all of the world’s landmasses came together in a single supercontinent, known as Pangaea. Not only did this stop the downwarping, it also dried the climate out (for the average point on land is farther from the ocean’s moist air in a supercontinent than in a group of smaller ones), meaning there were fewer swamps. Less coal was created, and more eroded than before. It was not until the Cretaceous, some time after Pangaea had broken apart again, that coal formation and preservation resumed. According to Dr Boyce’s hypothesis, it is therefore no coincidence that the second-most abundant source of coal today is rocks of the Cretaceous and the subsequent Caenozoic. If his hypothesis is correct, then, it is the grinding movement of the continents that is ultimately responsible for the Industrial Revolution. No continental drift, no coal. No coal, and humanity, if, indeed, such a species had evolved at all, might still be tilling the fields. ZIKA, a mosquito-borne virus that arrived in Brazil last May, is an avid traveller—and an increasingly feared guest. It has since found its way into 17 other countries in the Americas. Until October, Zika was not thought much of a threat: only a fifth of infected people fall ill, usually with just mild fever, rash, joint aches and red eyes. Since then, though, evidence has been piling up that it may cause birth defects in children and neurological problems in adults. On January 15th America’s Centres for Disease Control and Prevention (CDC) advised pregnant women not to travel to countries where Zika is circulating. The virus was first isolated in 1947, from a monkey in the Zika forest in Uganda. Since then it has caused small and sporadic outbreaks in parts of Africa and South-East Asia. In Brazil, for reasons yet unclear, it quickly flared into an epidemic after its arrival—by official estimates infecting as many as 1.5m people. Alarm bells started ringing in October, when doctors in Pernambuco, one of Brazil’s north-eastern states, saw a huge increase in babies born with microcephaly: an abnormally small head, often with consequent brain damage. In the next four months more than 3,500 cases of microcephaly were reported in Brazil. That compared with fewer than 200 a year in the five years before 2015. None of the known causes of the condition—which include genetic abnormalities, drugs, alcohol, rubella infection and exposure to some chemicals during pregnancy—seemed a plausible culprit. Last week, CDC scientists announced the best evidence so far that Zika can pass from mother to fetus: they found the virus in four Brazilian babies with microcephaly who had died in the womb or shortly after birth. Previously, Brazilian researchers had found Zika in the amniotic fluid of women carrying fetuses with microcephaly. There is another fear. After Zika arrived in Brazil, and also in El Salvador, both saw a sharp increase in severe neurological and autoimmune problems, including Guillain-Barré syndrome, which can lead to paralysis. These also surged in French Polynesia after Zika broke out there in 2013. Working out the extent to which Zika, alone or combined with other things, is to blame for any of this is tricky. Dengue and chikungunya—mosquito-borne viruses with similar symptoms—are common where Zika is making the rounds. According to Scott Weaver of the University of Texas, tests that spot Zika work only during its infectious phase, which lasts just a few days. After that, they are often useless if the patient has had dengue or been vaccinated against yellow fever. And only laboratories that can do sophisticated molecular tests are in the game in the first place. All of which means that most cases of Zika are missed, and many are misdiagnosed. Bearing these caveats in mind, researchers are mining the available surveillance data for answers. More solid results will come from prospective studies, set up recently, which are tracking pregnant women in Brazil, looking at whether those who catch Zika are more likely to have babies with birth defects. Researchers in America and other countries have begun work on a vaccine. Unlike the one for Ebola, though, which had been in the pipeline for a decade when the epidemic in West Africa began, a Zika vaccine is “at ground zero”, says Alan Barrett, also of the University of Texas. That is where potential antiviral drugs are, too. The spread of Zika makes attacking disease-carrying mosquitoes all the more important. Mostly, Zika is transmitted by Aedes aegypti, which is also the vector of dengue and yellow fever. This insect lives in tropical climes, but Aedes albopictus, found as far north as New York and Chicago, and in parts of southern Europe, can also do the job, though it is not clear how efficiently. A paper published last week in the Lancet shows where Zika could become endemic (see map). But places where air-conditioning, screened windows and mosquito control are the norm are unlikely to see outbreaks flare up. In December, Brazil decreed a national public-health emergency. This has removed bureaucratic hurdles to the purchase of insecticides for mosquito larvae, equipment for health workers and the like—and prompted speculation about whether this bureaucracy was necessary in the first place. It also enabled the deployment of the army to help 310,000 health workers in the mosquito-eradication drive. Brazil was declared free of A. aegypti in 1958, after a campaign that included regular fumigation and visits to ensure households got rid of standing water, where mosquitoes like to breed. Since then, the insect has bounced back. Might the fear of Zika help finish the job properly this time?THIRTY years ago a young haematologist called Richard Burt was training at Johns Hopkins University, in Baltimore. He noticed that after leukaemia patients had received a treatment to wipe out their immune systems, they needed to be re-immunised against diseases such as measles and mumps. Although the patients in question had been vaccinated as children, the therapy for their blood cancer had erased this cellular memory. Dr Burt turned to his teacher, William Burns, and ask whether the same might be possible in autoimmune diseases. “I could see a light go on in his eyes. ‘You should try it in multiple sclerosis’ he said.” Thus began decades of painstaking work. Multiple sclerosis (MS) happens when the body’s immune system learns to attack its own nerve fibres in the same way that it learns to attack invading pathogens. Nobody really understands what causes this misplaced learning. But Dr Burt’s idea did not depend on knowing that. He just wanted to wipe the memory out, in the way that the memory of a vaccination is wiped out by chemotherapy. By 2009 Dr Burt, now at Northwestern University, in Chicago, had proved that his treatment worked in patients with the most common form of the disease, relapsing remitting MS. The treatment involves using lower-dose chemotherapy to kill the white blood cells that are responsible for attacking nerve fibres, and then rebooting the immune system using stem cells collected from the patient before treatment began. Stem cells are the source from which more specialised cells develop. Those found in bone marrow, known as hematopoietic stem cells, produce the many different cells found in blood, including the white cells implicated in MS. In Dr Burt’s therapy such stem cells are extracted from a patient, stored until after the chemotherapy, and then infused back into him. Ten days later, he can go home. It is effective. Although there is a relapse rate of around 10% within five years, many who have been treated in randomised trials in Brazil, Britain and Sweden feel as though they have been cured. Proving they actually have been means waiting for the results of the trials, and watching how participants fare over many years. Already patients have been seen to improve for two years after treatment. This work should give drug companies some pause for thought. They are already facing criticism for the high prices of MS drugs. Moreover, though such drugs can slow the progression of the disease, they cannot do what the stem-cell therapy seems able to, which is to reverse it and improve patients’ quality of life—for example by allowing them to walk again. Last year a study published in Neurology found that the cost of MS drugs had risen five to seven times faster than the general inflation rate for prescription drugs. Medicines that cost $8,000 to $11,000 20 years ago were between $59,000 and $62,000 in 2013. On top of this come bills for doctors, MRI scans, blood draws and lab work. By contrast, Dr Burt reckons, a stem-cell transplant in America costs, all in, $120,000—and that sum would be lower in countries with less-expensive health-care systems. Indeed, doctors in Britain think the treatment should cost hospitals about £30,000 (just over $40,000). At those sorts of prices, stem cells are set to give MS drugs a real run for their money. More broadly, this is good news for proponents of stem-cell therapies in general. Leukaemia is mostly treated these days with hematopoietic stem cells. Delete Blood Cancer, a British charity, is encouraging people around the world to register to donate stem cells. Registration can be done with a cheek swab, to show the genetic make-up of the potential donor’s immune system. The cells themselves, if needed, are collected via a blood donation. And other conditions, too, seem susceptible to the stem-cell approach. Mesoblast, an Australian company, received approval last September for a stem-cell treatment for graft-versus-host disease, in which the transplanted tissue attacks the host. It has a number of other products in advanced trials, in areas such as chronic heart failure and chronic low-back pain. Stem-cell therapy, so long promised, is starting to become a reality.   Correction: In an earlier version of this piece we said that in graft-versus-host disease the immune system rejects transplanted tissue. This is the wrong way around. Sorry.Friends to the end HOW many Facebook friends do you have? For some, the answer can be a signal of social success, and the numbers claimed can be enormous: Facebook permits 5,000 of them (though these might include products and companies as well as people). But Robin Dunbar, a psychologist at Oxford University, has long reckoned that claims of vast numbers of Facebook friends do not say much about actual human relationships. This week, as he describes in a paper in Royal Society Open Science, he is even more certain. Dr Dunbar is the eponymous originator of Dunbar’s number, a rough measure of the number of stable relationships that individuals can maintain. He came up with it in 1993, when he was studying the brains of social primates. He found a correlation between the average size of each species’s neocortex (a recently evolved part of the brain) and that of their social groups. Extrapolating the results to humans, he reckoned, meant they should have social circles—of close friends and relatives, and frequently seen acquaintances—of about 150 people. And that is what he found. From the sizes of Neolithic villages to the centuries of Roman legions, humans seem to have organised themselves in the past into groups of 100-200. Things have changed a bit since Neolithic and Roman times, though, and many wonder what effects modern technology might have on the size of such circles. Perhaps there is indeed a cognitive limit, imposed by the brain’s internal architecture, on how large a social structure can be maintained. But there may also be another limit: time. Maintaining 150 friendships face-to-face consumes a lot of that. Cobbling together many times this number of connections online, though, is a doddle. Previous attempts to decide between these possibilities have tended to come down on the cognitive-limit side of the fence. But they have been criticised for looking at unrepresentative groups of people: students (inevitably), scientists and particularly heavy users of social networks. The latest try, in which Dr Dunbar piggybacked on a survey organised by a biscuit-maker, has overcome that. It is the first national-scale, randomly sampled study to investigate the matter. The survey asked 2,000 people, chosen because they were regular social-network users, and a further 1,375 adults in full-time employment, who might or might not have been such users, how many friends they had on Facebook. The results showed, to no surprise whatsoever on the part of Dr Dunbar, that the average number of Facebook friends in the two groups were Dunbar-sized numbers: 155 and (when those who did not use Facebook at all were excluded) 187, respectively. Other details matched Dr Dunbar’s earlier work, too. This described a pair of smaller socially relevant numbers—a support clique (people you would rely on in a crisis) of about five and a sympathy group (those you would call close friends) of about 15. Such cliques and groups turned up in detailed answers to questions about Facebook users’ relations with others. These results, then, confirm that what constrains an individual’s number of friends is neurological. Even though social networks like Facebook could help people handle far more social interactions than Dunbar’s number describes, it seems the human brain simply cannot keep up.
